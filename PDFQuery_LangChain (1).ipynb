{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgOhk8U4Rpl"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        " Querying PDF With Astra and LangChain\n",
        "\n",
        "### A question-answering demo using Astra DB and LangChain, powered by Vector Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfJKgRM4Rpo"
      },
      "source": [
        "#### Pre-requisites:\n",
        "\n",
        "You need a **_Serverless Cassandra with Vector Search_** database on [Astra DB](https://astra.datastax.com) to run this demo. As outlined in more detail [here](https://docs.datastax.com/en/astra-serverless/docs/vector-search/quickstart.html#_prepare_for_using_your_vector_database), you should get a DB Token with role _Database Administrator_ and copy your Database ID: these connection parameters are needed momentarily.\n",
        "\n",
        "You also need an [OpenAI API Key](https://cassio.org/start_here/#llm-access) for this demo to work.\n",
        "\n",
        "#### What you will do:\n",
        "\n",
        "- Setup: import dependencies, provide secrets, create the LangChain vector store;\n",
        "- Run a Question-Answering loop retrieving the relevant headlines and having an LLM construct the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_FeN-Ep4Rpp"
      },
      "source": [
        "Install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk0qUhJUQrkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab43ec23-b06f-4805-d8b2-a897b0412dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQN-L2J4Rpq"
      },
      "source": [
        "Import the packages you'll need:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0qELYrckzqc",
        "outputId": "66b31541-343a-40a6-9f5b-dab6f3c35d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.12)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.136)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.3 marshmallow-3.23.0 mypy-extensions-1.0.0 pydantic-settings-2.6.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxJt6cZllAfh",
        "outputId": "64771f49-fe65-4e22-9cd0-b82e975301af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.3.12)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq) (0.1.136)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq) (3.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq) (2.2.3)\n",
            "Downloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.11.0 langchain-groq-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4dEHiyelL0y",
        "outputId": "d3497558-6096-4ca5-9403-2ffffbdd060a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.24.7)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.3.12)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
            "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.1.136)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.10.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers, langchain-huggingface\n",
            "Successfully installed langchain-huggingface-0.1.0 sentence-transformers-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4qBIihE4Rpq"
      },
      "outputs": [],
      "source": [
        "# LangChain components to use\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Support for dataset retrieval with Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
        "# you will also initialize the DB connection:\n",
        "import cassio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs76OPQ6JyD",
        "outputId": "68ab390b-c2ea-4d40-85be-83cfe01e9948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "1itBNL1v6N9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu2UauiC4Rpr"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqpM6I854Rpr"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:uSOLiBkvFWHsfZwbKstrLzag:da3f46edf90863135934f19dcad6210b0fde0a5e7a9f079688d04cb6901522e0\" # enter the \"AstraCS:...\" string found in in your Token JSON file\n",
        "ASTRA_DB_ID = \"063b8bdc-c1e3-45c2-9f3a-baac206bb054\" # enter your Database ID\n",
        "\n",
        "groq_api_key=\"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\"\n",
        "HF_TOKEN=\"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1cmD5EF4Rpr"
      },
      "source": [
        "#### Provide your secrets:\n",
        "\n",
        "Replace the following with your Astra DB connection details and your OpenAI API key:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the path of  pdf file/files.\n",
        "pdfreader = PdfReader('3_Data_Warehouse.pdf')"
      ],
      "metadata": {
        "id": "waVKJW-n6jqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content"
      ],
      "metadata": {
        "id": "42BKuFRO6meP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "vR41Iq-4ZHnG",
        "outputId": "dcc8cc82-ed98-4f6b-c23b-9db463a0df79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data Warehouse \\nConcepts and Techniques\\nMunmun Bhattacharya \\nDept. of information Technology\\nJadavpur University, Salt Lake Campus2Data Warehousing and OLAP: Contents\\n•Data Warehouse: Basic Concepts\\n•Data Warehouse Modeling: Data Cube and OLAP\\n•Data Warehouse Design and Usage\\n•Data Warehouse Implementation\\n•Summary3What is a Data Warehouse?\\n•Defined in many different ways, but not rigorously.\\n–A decision support database that is maintained separately \\nfrom the organization’s operational database\\n–Support information processing  by providing a solid platform \\nof consolidated, historical data for analysis.\\n•“A data warehouse is a subject -oriented , integrated , time-\\nvariant , and nonvolatile  collection of data in support of \\nmanagement’s decision -making process.” —W. H. Inmon\\n•Data warehousing:\\n–The process of constructing and using data warehouses4Data Warehouse\\n•Data warehouses generalize  and consolidate  data in \\nmultidimensional space.\\n•Moreover, data warehouses provide online analytical \\nprocessing (OLAP) tools for the interactive analysis of \\nmultidimensional data of varied granularities , which facilitates \\neffective data generalization and data mining.\\n•Many other data mining functions, such as association, \\nclassification, prediction, and clustering , can be integrated with \\nOLAP operations to enhance interactive mining of knowledge \\nat multiple levels of abstraction.\\n•Data warehousing and OLAP form an essential step in the \\nknowledge discovery process.5Data Warehouse\\n•A data warehouse is a repository of information collected \\nfrom multiple sources, stored under a unified schema, and \\nthat usually resides at a single site.\\n•Data warehouse is constructed via processes of data \\ncleaning , data integration , data transformation , data loading , \\nand periodic data refreshing .\\n•To facilitate decision making, data warehouse is organized \\naround the major subjects [e.g., such as customer, item, \\nsupplier, etc.]\\n–Data are to be stored to provide information from historical \\nperspective.\\n–E.g., rather storing details of each transactions, data \\nwarehouse may store a summary of the transaction.6Data Warehouse\\nFramework of data warehouse\\n7Data Warehouse\\nSubject -oriented!!\\n–A data warehouse  is organized  around  the major  subjects  of the \\nenterprise  instead  of application . \\n Non-Volatile!!\\n–The data in the data warehouse  is not updated  or changed  in real \\ntime but is loaded  and refreshed  from operational  systems  on a \\nregular  basis,  and accessed  for queries . \\n Time -Variant!!\\n–In a data warehouse,  data are stored  to provide  a historical  \\nperspective . \\n Integrated!!\\n–A data warehouse  is usually  constructed  by integrating  multiple  \\nheterogeneous  sources  such  as flat files, relational  databases,  \\nand OLTP  files and so on. 8Data Warehouse —Subject -Oriented\\n•Organized around major subjects, such as customer, \\nproduct, sales etc.\\n•Instead of concentrating on the day -to-day operations and \\ntransaction processing of an organization, it focusing on the \\nmodeling and analysis of data for decision makers, not on \\ndaily operations or transaction processing\\n•Provide a simple and concise  view around particular subject \\nissues by excluding data that are not useful in the decision \\nsupport process9Data Warehouse —Integrated\\n•Constructed by integrating multiple, heterogeneous data \\nsources\\n–relational databases, flat files, on -line transaction records \\netc.\\n•Data cleaning and data integration techniques are applied to\\n–Ensure consistency in naming conventions, encoding \\nstructures, attribute measures, etc. among different data \\nsources\\n•E.g., Hotel price: currency, tax, breakfast covered, etc.\\n–When data is moved to the warehouse, it is converted.  10Data Warehouse —Time Variant\\n•The time horizon for the data warehouse is significantly \\nlonger than that of operational systems\\n–Operational database: current value data\\n–Data warehouse data: provide information from a \\nhistorical perspective (e.g., past 5 -10 years sometime \\nmore than that)\\n•Every key structure in the data warehouse\\n–Contains an element of time, explicitly or implicitly\\n–But the key of operational data may or may not contain \\n“time element”11Data Warehouse —Nonvolatile\\n•A physically separate store  of data transformed from \\nthe application data in operational environment\\n•Operational update of data does not occur  in the data \\nwarehouse environment (because new data are \\nalways appended during load followed by refresh)\\n–Does not require transaction processing, recovery, \\nand concurrency control mechanisms\\n–Requires only two operations in data accessing: \\n•initial loading of data  and access of data12Why a Separate Data Warehouse?\\n•High performance for both systems\\n–DBMS — tuned for OLTP: access methods, indexing, hashing, \\noptimizing queries, concurrency control, recovery etc.\\n–Warehouse —tuned for OLAP: complex OLAP queries, \\nmultidimensional view, consolidation\\n–If OLAP queries are used in operation databases then performance \\nwill degrade.\\n•Different functions and different data:\\n–missing data : Decision Support requires historical data which \\noperational DBs do not typically maintain.\\n–data consolidation :  DS requires consolidation (aggregation, \\nsummarization) of data from heterogeneous sources.\\n–data quality : different sources typically use inconsistent data \\nrepresentations, codes and formats which have to be reconciled13Why a Separate Data Warehouse?\\n❖Why Concurrency control and Recovery mechanisms  \\nare not required for OLAP?\\n•Concurrency control and recovery mechanisms (e.g., \\nlocking and logging) are required to ensure the consistency \\nand robustness of transactions. An OLAP query often \\nneeds read -only access of data records for summarization \\nand aggregation . \\n•Concurrency control and recovery mechanisms , if applied \\nfor such OLAP operations, may jeopardize the execution of \\nconcurrent transactions and thus substantially reduce the \\nthroughput of an OLTP system.14Why data warehouse?\\n•The traditional approach in heterogeneous database system is to \\nbuild wrappers and integrators, on the top of the multiple, \\nheterogeneous databases.\\n–Here the given query is translated into appropriate form for \\nindividual databases\\n–When a query is posed to a client site, a metadata dictionary is used \\nto translate the query into queries appropriate for the individual \\nheterogeneous sites involved. These queries are then mapped and \\nsent to local query processors. The results returned from the \\ndifferent sites are integrated into a global answer set\\n–This query -driven approach requires complex information filtering \\nand integration processes, and competes with local sites for \\nprocessing resources\\n–It is inefficient and potentially expensive for frequent queries, \\nespecially queries requiring aggregations.\\n•Warehousing is an alternative approach15Why data warehouse?\\n•Data warehousing employs an update driven approach in which \\ninformation from multiple, heterogeneous sources is integrated \\nin advance and stored in a warehouse for direct querying and \\nanalysis. \\n•Unlike OLTP databases, data warehouses do not contain the \\nmost current information. \\n•Data warehouse brings high performance to the integrated \\nheterogeneous database system because data are copied, \\npreprocessed, integrated, annotated, summarized, and \\nrestructured into one semantic data store .\\n⁃Query processing in data warehouses does not interfere with the \\nprocessing at local sources\\n⁃Data warehouses can store and integrate historic information and \\nsupport complex multidimensional queries. As a result, data \\nwarehousing has become popular in industry16Comparison Between Data Warehouses and OLTP \\nSystems\\nOLTP Systems Data Warehouses\\nIt holds current data and detailed \\ndata. It holds detailed data, historical data \\nand summarized data.\\nData is dynamic. Data is largely static.\\nIn this system, processing is \\nrepetitive.In this system, processing is ad hoc, \\nunstructured and heuristic.\\nTransaction throughput is very high. Transaction throughput is low to \\nmedium.\\nIn this case, usage pattern is \\npredictableIn this case, usage pattern is \\nunpredictable.\\nData is application oriented. Data is subject oriented.\\nIt supports day -to-day decisions. It supports strategic decisions.\\nIt serves large number of operational \\nusers.It serves relatively low number of \\nmanagerial users.17OLTP vs. OLAP\\n18OLTP vs. OLAP\\n•Users and system orientation: An OLTP system is customer -\\noriented  and is used for transaction and query processing by \\nclerks, clients, and information technology professionals. \\n–An OLAP system is market -oriented and is used for data analysis \\nby knowledge workers, including managers, executives, and \\nanalysts.\\n•Data contents: An OLTP system manages current data that, \\ntypically, are too detailed to be easily used for decision making.\\n–An OLAP system manages large amounts of historic data , \\nprovides facilities for summarization and aggregation, and stores \\nand manages information at different levels of granularity. \\n–These features make the data easier to use for informed decision \\nmaking.19OLTP vs. OLAP\\n•Database design: An OLTP system usually adopts an entity -\\nrelationship (ER) data model  and an application -oriented \\ndatabase design. An OLAP system typically adopts either a \\nstar or a snowflake model and a subject -oriented database \\ndesign\\n•View:  An OLTP system focuses mainly on the current data \\nwithin an enterprise or department , without referring to historic \\ndata or data in different organizations. \\n–In contrast, an OLAP system often spans multiple versions of a \\ndatabase schema , due to the evolutionary process of an organization. \\nOLAP systems also deal with information that originates from different \\norganizations, integrating information from many data stores. Because \\nof their huge volume, OLAP data are stored on multiple storage media.20OLTP vs. OLAP\\n•Access patterns: The access patterns of an OLTP system \\nconsist mainly of short, atomic transactions . \\n•Such a system requires concurrency control and recovery \\nmechanisms. \\n•However, accesses to OLAP systems are mostly read -only \\noperations (because most data warehouses store historic \\nrather than up -to-date information), although many could be \\ncomplex queries21OLTP vs. OLAP (Summary)\\n•OLTP mainly manages current data; OLAP manages large \\namount of historical data.\\n•OLTP system consist mainly of short, atomic transactions \\nand such a system requires concurrent control, recovery \\nmechanism, etc.\\n•OLAP system: mainly read -only operations - and dominated \\nby ad hoc complex queries.\\n•OLAP applications bulk of data can be represented in \\nmultidimensional data model.22Architecture of a Data Warehouse\\nIt has a three -tier architecture  and consists  of the \\nfollowing  components :\\n•Operational  data source\\n•Load  manager\\n•Query  manager\\n•Warehouse  manager\\n•Detailed  data\\n•Summarized  data\\n•Archive/Backup  data\\n•Metadata\\n•End-users  Access  Tools\\n•Data  Warehouse  Background  Processes  23Data Warehouse: A Multi -Tiered Architecture (3 tier)\\n24Data Warehouse: A Multi -Tiered Architecture (3 tier)\\n◼Bottom Tier: The bottom tier is a warehouse database server that is \\nalmost always a relational database system. \\n◼Back -end tools and utilities are used to feed data into the bottom tier \\nfrom operational databases or other external sources (e.g., customer \\nprofile information provided by external consultants).\\n◼These tools and utilities perform data extraction, cleaning, and \\ntransformation  (e.g., to merge similar data from different sources into \\na unified format), as well as load and refresh functions  to update the \\ndata warehouse. \\n◼The data are extracted using application program interfaces known \\nas gateways. A gateway (e.g., ODBC, OLEDB, JDBC etc.) is \\nsupported by the underlying DBMS and allows client programs to \\ngenerate SQL code to be executed at a server. \\n◼This tier also contains a metadata repository , which stores \\ninformation about the data warehouse and its contents.\\n 25◼Middle Tier: The middle tier is an OLAP server that is typically \\nimplemented using either -\\n1)a relational OLAP(ROLAP) model (i.e., an extended \\nrelational DBMS that maps operations on multidimensional \\ndata to standard relational operations); or \\n2)a multidimensional OLAP (MOLAP) model (i.e., a special -\\npurpose server that directly implements multidimensional \\ndata and operations). \\n◼Top Tier: The top tier is a front -end client layer, which contains \\nquery and reporting tools, analysis tools, and/or data mining \\ntools (e.g., trend analysis, prediction, and so on).Data Warehouse: A Multi -Tiered Architecture (3 tier)26Models of Data Warehouse\\n•Enterprise warehouse : \\n–Collects all of the information about subjects spanning the entire \\norganization. It provides corporate -wide data integration, usually from one \\nor more operational systems or external information providers (with \\ndetailed data as well as summarized data, having size of few gigabytes to \\nhundreds of gigabytes, terabytes, or beyond).\\n•Data Mart:\\n–A subset of corporate -wide data that is of value to a specific groups of \\nusers.  Its scope is confined to specific, selected groups, such as \\nmarketing data mart (usually implemented on low -cost departmental \\nservers). Two types of data mart: Independent  (sourced from operational \\nsystems or external information) and Dependent  (directly from enterprise \\nwarehouse)\\n•Virtual warehouse:\\n–A set of views over operational databases. Only some of the possible \\nsummary views may be materialized27Data Mart\\nDefinition : A subset  of a data warehouse  that supports  \\nthe requirements  of a particular  department  or business  \\nfunction  is called  data mart.\\n   Characteristics of Data Mart: \\n–A data mart focuses  on only the requirements  of users  \\nassociated  with one department  or business  function .\\n–A data mart does  not contain  detailed  operational  data.\\n–A data mart contains  less data with compared  to a data \\nwarehouse,  thus,  it is easier  to understand  and \\nnavigate . 28Approach of Data Warehouse\\nRecommended approach for data warehouse development in incremental and \\nevolutionary manner29Extraction, Transformation, & Loading (ETL)\\n•Data extraction\\n–Gathers data from multiple, heterogeneous, and external \\nsources\\n•Data cleaning\\n–Detects errors in the data and rectify them when possible\\n•Data transformation\\n–Converts data from legacy or host format to warehouse \\nformat\\n•Load\\n–Sorts, summarizes, consolidates, computes views, \\nchecks integrity, and builds indices and partitions\\n•Refresh\\n–Propagates the updates from the data sources to the \\nwarehouse30Metadata Repository\\nMeta data  is the data defining warehouse objects. Meta data  \\nrepositories stores:\\n•Description of the structure  of the data warehouse\\n–schema, view, dimensions, hierarchies, derived data defn, data \\nmart locations and contents\\n•Operational  meta -data\\n–data lineage (history of migrated data and transformation path), \\ncurrency of data (active, archived, or purged), monitoring \\ninformation (warehouse usage statistics, error reports, audit trails)\\n•The algorithms  used for summarization\\n•The mapping  from operational environment to the data warehouse\\n•Data related to system performance\\n–warehouse schema, view and derived data definitions\\n•Business metadata\\n–business terms and definitions, ownership of data, charging policies31OLAP Tools\\n•Multi -dimensional  OLAP  (MOLAP)  – These  tools  utilize  \\nspecialized  data structures  and multidimensional  database  \\nmanagement  systems  to organize,  navigate  and analysis  \\ndata.\\n \\n•Relational  OLAP  (ROLAP)  - These  tools  support  RDBMS  \\nproducts  directly  through  a dictionary  layer  of metadata,  \\nthereby  avoiding  the requirement  to create  a static  \\nmultidimensional  data structure . \\n•Hybrid  OLAP  (HOLAP)  - These  tools  provide  limited  \\nanalysis  capability,  either  directly  against  RDBMS  products,  \\nor by leveraging  an intermediate  MOLAP  server . 32On-line Analytical Processing (OLAP)\\nThe on-line analytical  processing  represents  a technology  that uses  a \\nmultidimensional  view of aggregated  data to provide  a quick  access  \\nto strategic  information  for performing  advanced  analysis . Codd’s  \\ntwelve  rules  for selecting  OLAP  tools :\\n–Multidimensional  conceptual  view \\n–Transparency\\n–Accessibility\\n–Consistent  reporting  performance  \\n–Client/Server  architecture\\n–Generic  dimensionality  \\n–Dynamic  sparse  matrix  handling  \\n–Multi -user support\\n–Unrestricted  cross -dimensional  operations  \\n–Intuitive  data manipulation\\n–Flexible  reporting\\n–Unlimited  dimensions  and aggregation  levels  33•A data cube allows data to be modeled and viewed in multiple \\ndimensions. It is defined by dimensions  and facts .\\n–Dimension: are the perspectives or entities w.r.t.  which the \\norganization want to keep records. For example, time, branch, item \\netc. A dimension table for item may contain the attributes item name, \\nbrand, and type .\\n–Facts: are numeric measures. It contains the names of the facts, or \\nmeasures, as well as keys to each of the related dimension tables. \\nExamples of facts for a sales data warehouse include dollars sold, \\nunits sold, and amount budgeted.\\n•In multiple data model, data are organized into multiple \\ndimensions, an each dimension contains multiple levels of \\nabstractions.\\n•Various multidimensional models are: star schema, snowflake \\nschema, and fact constellation\\n•This allows users to view data from different angles.\\n•In data warehousing the data cube is n -dimensional.Data Cube (as a multidimensional data)34Data Cube (as a multidimensional data)\\n•A number of OLAP data cube operations exists to materialize \\nthe different views.\\n•A data warehouse  is based on a multidimensional data \\nmodel  which views data in the form of a data cube\\n•A data cube, such as sales , allows data to be modeled and \\nviewed in multiple dimensions\\n–Dimension tables , such as item ( item_name , brand, type), time (day, \\nweek, month, quarter, year), and sales ( item, branch, location ) \\n–The fact table contains the names of the facts, or measures, (such as \\ndollars_sold ) and keys to each of the related dimension tables35•A cuboid  can be generated for each of the possible subsets of \\nthe given set of dimensions.\\n•The result would form a lattice of cuboids, each showing the \\ndata at a different level of summarization, or group -by. The \\nlattice  of cuboids forms a data cube .\\n•The lattice of cuboids is then referred to as a data cube.\\n•Representing data cubes (in previous slides, e.g., 2 -D data \\ncube, 3 -D data cube, 4 -D data cube etc.) are called data \\ncuboids . In data warehousing literature, an n-D base cube \\n(holding the lowest level of summarization ) is called a base \\ncuboid . \\n•The top most 0-D cuboid , which holds the highest -level of \\nsummarization , is called the apex cuboid .  Data Cube (as a multidimensional data)36Cube: A Lattice of Cuboids\\n37Conceptual Modeling of Data Warehouses\\n•The E -R data model is commonly used in the design of relational \\ndatabases, where a database schema consists of a set of entities \\nand the relationships between them\\n•Multidimensional modeling of data warehouses requires: \\ndimensions & measures (named as facts)\\n–Star schema : A fact table in the middle connected to a set of \\ndimension tables. Attributes within a dimension table may form either \\na hierarchy (total order) or a lattice (partial order). \\n–Snowflake schema :  It is a variant of the star schema model. A \\nrefinement of star schema where some dimensional hierarchy is \\nnormalized  into a set of smaller dimension tables (to reduce \\nredundancies), forming a shape similar to snowflake\\n–Fact constellations :  Multiple fact tables share dimension tables , \\nviewed as a collection of stars, therefore called galaxy schema  or fact \\nconstellation 38A Concept Hierarchy: Dimension  (location)\\nMexicoall\\nEurope North_America\\nCanada Spain Germany\\nVancouver\\nM. Wind L. Chan...\\n... ...\\n... ...\\n...all\\nregion\\nofficecountry\\nToronto Frankfurt city39Data Cube Measurement : Three Categories\\n(based on the kind of aggregate functions used)\\n•Distributive : Data are partitioned into n sets. Then a function (say \\naggregate ) is applied to each partition, resulting in n aggregate \\nvalues. If the result derived by applying the function to n aggregate \\nvalues is the same as that derived by applying the function on all the \\ndata without partitioning\\n•E.g., count(), sum(), min(), max()\\n•Algebraic : An aggregate function is algebraic if it can be computed \\nby an algebraic function with M arguments (where  M is a bounded \\ninteger), each of which is obtained by applying a distributive \\naggregate function\\n•E.g.,  avg(), min_N (), max_N (), standard_deviation ()\\n•Holistic : An aggregate function is holistic if there is no constant \\nbound on the storage size needed to describe a sub -aggregate  \\n•E.g., median(), mode(), rank()40Multidimensional Data\\n•Sales volume as a function of product, month, and region\\nProduct\\nMonthDimensions: Product, Location, Time\\nHierarchical summarization paths\\nIndustry   Region          Year\\nCategory   Country  Quarter\\nProduct      City     Month    Week\\n                   Office         Day\\n Hierarchies for product       Lattice for time\\n            & location41A Sample Data Cube\\nTotal annual sales\\nof  TVs in U.S.A.Date\\nCountry\\nAll, All, AllsumsumTV\\nVCRPC1Qtr 2Qtr 3Qtr 4Qtr\\nU.S.A\\nCanada\\nMexico\\nsum42Cuboids Corresponding to the Cube\\nall\\nproductdatecountry\\nproduct,date\\nproduct,countrydate, country\\nproduct, date, country0-D (apex ) cuboid\\n1-D cuboids\\n2-D cuboids\\n3-D (base ) cuboid43OLAP Operations\\n•Roll up (drill -up): (to) summarize data\\n–by climbing up hierarchy or by dimension reduction\\n•Drill down (roll down):  reverse of roll -up\\n–from higher level summary to lower level summary or detailed \\ndata, or introducing new dimensions\\n•Slice and dice:  project (synonymous to projection) and select  \\n•Pivot (rotate):  rotate the data axes in view to provide an \\n         alternative data presentation\\n–reorient the cube (i.e., rotating the axes in a 3 -D cube), \\nvisualization, transforming 3D cube to series of 2D planes etc.\\n•Other OLAP operations:\\n–drill across:  involving (across) more than one fact table\\n–drill through:  through the bottom level of the cube to its back -end \\nrelational tables (using SQL)44Data Warehouse Usage\\n•Three kinds of data warehouse applications:\\n–Information processing\\n•supports querying, basic statistical analysis, and reporting \\nusing crosstabs, tables, charts and graphs\\n–Analytical processing\\n•multidimensional analysis of data warehouse data\\n•supports basic OLAP operations, slice -dice, drilling, pivoting\\n–Data mining\\n•knowledge discovery from hidden patterns \\n•supports associations, constructing analytical models, \\nperforming classification and prediction, and presenting the \\nmining results using visualization tools45Problems in Data Warehousing\\n•Complexity of data integration\\n•Hidden problems with data sources\\n•Required data not captured\\n•Underestimation of time for data loading \\n•Increased storage space\\n•High maintenance\\n•Long -duration projects\\n•Data homogenization 46Benefits of Data warehousing \\n•Returns on investment in data warehousing is very high. \\n•The competitive advantage is gained from data warehousing by \\nallowing decision -makers access to data that can reveal \\npreviously unavailable, unknown and not captured. \\n•Data warehousing improves the productivity of corporate \\ndecision -makers. A data warehouse helps business managers \\nto perform more substantive, accurate and consistent analysis.\\n•Data warehousing reduces the redundant processing and \\nprovides software to support overlapping decision support \\napplications. \\n•A data warehouse facilitates better business intelligence by \\nincreased quality and flexibility of market analysis available \\nthrough multi -level data structures. \\n•It also facilitates business process reengineering. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S0GgIQs4Rps"
      },
      "source": [
        "Initialize the connection to your database:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFBR5HnZSPmK"
      },
      "outputs": [],
      "source": [
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex7NxZYb4Rps"
      },
      "source": [
        "Create the LangChain embedding and LLM objects for later usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TavS0AK2SLrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e0c1f1-e79c-4dcf-a6c6-c803317bd109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HMMx5Pm4Rpt"
      },
      "source": [
        "Create your LangChain vector store ... backed by Astra DB!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg9VAk4USQvU"
      },
      "outputs": [],
      "source": [
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,\n",
        "    table_name=\"qa_mini_demo\",\n",
        "    session=None,\n",
        "    keyspace=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "9FMAhKr77AVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8BDHAyT7Gjr",
        "outputId": "438cbf69-6935-4d86-cdfc-90fd84bf85eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data Warehouse \\nConcepts and Techniques\\nMunmun Bhattacharya \\nDept. of information Technology\\nJadavpur University, Salt Lake Campus2Data Warehousing and OLAP: Contents\\n•Data Warehouse: Basic Concepts\\n•Data Warehouse Modeling: Data Cube and OLAP\\n•Data Warehouse Design and Usage\\n•Data Warehouse Implementation\\n•Summary3What is a Data Warehouse?\\n•Defined in many different ways, but not rigorously.\\n–A decision support database that is maintained separately \\nfrom the organization’s operational database\\n–Support information processing  by providing a solid platform \\nof consolidated, historical data for analysis.\\n•“A data warehouse is a subject -oriented , integrated , time-\\nvariant , and nonvolatile  collection of data in support of \\nmanagement’s decision -making process.” —W. H. Inmon',\n",
              " '•“A data warehouse is a subject -oriented , integrated , time-\\nvariant , and nonvolatile  collection of data in support of \\nmanagement’s decision -making process.” —W. H. Inmon\\n•Data warehousing:\\n–The process of constructing and using data warehouses4Data Warehouse\\n•Data warehouses generalize  and consolidate  data in \\nmultidimensional space.\\n•Moreover, data warehouses provide online analytical \\nprocessing (OLAP) tools for the interactive analysis of \\nmultidimensional data of varied granularities , which facilitates \\neffective data generalization and data mining.\\n•Many other data mining functions, such as association, \\nclassification, prediction, and clustering , can be integrated with \\nOLAP operations to enhance interactive mining of knowledge \\nat multiple levels of abstraction.',\n",
              " 'classification, prediction, and clustering , can be integrated with \\nOLAP operations to enhance interactive mining of knowledge \\nat multiple levels of abstraction.\\n•Data warehousing and OLAP form an essential step in the \\nknowledge discovery process.5Data Warehouse\\n•A data warehouse is a repository of information collected \\nfrom multiple sources, stored under a unified schema, and \\nthat usually resides at a single site.\\n•Data warehouse is constructed via processes of data \\ncleaning , data integration , data transformation , data loading , \\nand periodic data refreshing .\\n•To facilitate decision making, data warehouse is organized \\naround the major subjects [e.g., such as customer, item, \\nsupplier, etc.]\\n–Data are to be stored to provide information from historical \\nperspective.',\n",
              " 'around the major subjects [e.g., such as customer, item, \\nsupplier, etc.]\\n–Data are to be stored to provide information from historical \\nperspective.\\n–E.g., rather storing details of each transactions, data \\nwarehouse may store a summary of the transaction.6Data Warehouse\\nFramework of data warehouse\\n7Data Warehouse\\nSubject -oriented!!\\n–A data warehouse  is organized  around  the major  subjects  of the \\nenterprise  instead  of application . \\n Non-Volatile!!\\n–The data in the data warehouse  is not updated  or changed  in real \\ntime but is loaded  and refreshed  from operational  systems  on a \\nregular  basis,  and accessed  for queries . \\n Time -Variant!!\\n–In a data warehouse,  data are stored  to provide  a historical  \\nperspective . \\n Integrated!!',\n",
              " 'regular  basis,  and accessed  for queries . \\n Time -Variant!!\\n–In a data warehouse,  data are stored  to provide  a historical  \\nperspective . \\n Integrated!!\\n–A data warehouse  is usually  constructed  by integrating  multiple  \\nheterogeneous  sources  such  as flat files, relational  databases,  \\nand OLTP  files and so on. 8Data Warehouse —Subject -Oriented\\n•Organized around major subjects, such as customer, \\nproduct, sales etc.\\n•Instead of concentrating on the day -to-day operations and \\ntransaction processing of an organization, it focusing on the \\nmodeling and analysis of data for decision makers, not on \\ndaily operations or transaction processing\\n•Provide a simple and concise  view around particular subject \\nissues by excluding data that are not useful in the decision',\n",
              " 'daily operations or transaction processing\\n•Provide a simple and concise  view around particular subject \\nissues by excluding data that are not useful in the decision \\nsupport process9Data Warehouse —Integrated\\n•Constructed by integrating multiple, heterogeneous data \\nsources\\n–relational databases, flat files, on -line transaction records \\netc.\\n•Data cleaning and data integration techniques are applied to\\n–Ensure consistency in naming conventions, encoding \\nstructures, attribute measures, etc. among different data \\nsources\\n•E.g., Hotel price: currency, tax, breakfast covered, etc.\\n–When data is moved to the warehouse, it is converted.  10Data Warehouse —Time Variant\\n•The time horizon for the data warehouse is significantly \\nlonger than that of operational systems',\n",
              " '–When data is moved to the warehouse, it is converted.  10Data Warehouse —Time Variant\\n•The time horizon for the data warehouse is significantly \\nlonger than that of operational systems\\n–Operational database: current value data\\n–Data warehouse data: provide information from a \\nhistorical perspective (e.g., past 5 -10 years sometime \\nmore than that)\\n•Every key structure in the data warehouse\\n–Contains an element of time, explicitly or implicitly\\n–But the key of operational data may or may not contain \\n“time element”11Data Warehouse —Nonvolatile\\n•A physically separate store  of data transformed from \\nthe application data in operational environment\\n•Operational update of data does not occur  in the data \\nwarehouse environment (because new data are',\n",
              " 'the application data in operational environment\\n•Operational update of data does not occur  in the data \\nwarehouse environment (because new data are \\nalways appended during load followed by refresh)\\n–Does not require transaction processing, recovery, \\nand concurrency control mechanisms\\n–Requires only two operations in data accessing: \\n•initial loading of data  and access of data12Why a Separate Data Warehouse?\\n•High performance for both systems\\n–DBMS — tuned for OLTP: access methods, indexing, hashing, \\noptimizing queries, concurrency control, recovery etc.\\n–Warehouse —tuned for OLAP: complex OLAP queries, \\nmultidimensional view, consolidation\\n–If OLAP queries are used in operation databases then performance \\nwill degrade.\\n•Different functions and different data:',\n",
              " 'multidimensional view, consolidation\\n–If OLAP queries are used in operation databases then performance \\nwill degrade.\\n•Different functions and different data:\\n–missing data : Decision Support requires historical data which \\noperational DBs do not typically maintain.\\n–data consolidation :  DS requires consolidation (aggregation, \\nsummarization) of data from heterogeneous sources.\\n–data quality : different sources typically use inconsistent data \\nrepresentations, codes and formats which have to be reconciled13Why a Separate Data Warehouse?\\n❖Why Concurrency control and Recovery mechanisms  \\nare not required for OLAP?\\n•Concurrency control and recovery mechanisms (e.g., \\nlocking and logging) are required to ensure the consistency \\nand robustness of transactions. An OLAP query often',\n",
              " 'are not required for OLAP?\\n•Concurrency control and recovery mechanisms (e.g., \\nlocking and logging) are required to ensure the consistency \\nand robustness of transactions. An OLAP query often \\nneeds read -only access of data records for summarization \\nand aggregation . \\n•Concurrency control and recovery mechanisms , if applied \\nfor such OLAP operations, may jeopardize the execution of \\nconcurrent transactions and thus substantially reduce the \\nthroughput of an OLTP system.14Why data warehouse?\\n•The traditional approach in heterogeneous database system is to \\nbuild wrappers and integrators, on the top of the multiple, \\nheterogeneous databases.\\n–Here the given query is translated into appropriate form for \\nindividual databases',\n",
              " 'build wrappers and integrators, on the top of the multiple, \\nheterogeneous databases.\\n–Here the given query is translated into appropriate form for \\nindividual databases\\n–When a query is posed to a client site, a metadata dictionary is used \\nto translate the query into queries appropriate for the individual \\nheterogeneous sites involved. These queries are then mapped and \\nsent to local query processors. The results returned from the \\ndifferent sites are integrated into a global answer set\\n–This query -driven approach requires complex information filtering \\nand integration processes, and competes with local sites for \\nprocessing resources\\n–It is inefficient and potentially expensive for frequent queries, \\nespecially queries requiring aggregations.',\n",
              " 'and integration processes, and competes with local sites for \\nprocessing resources\\n–It is inefficient and potentially expensive for frequent queries, \\nespecially queries requiring aggregations.\\n•Warehousing is an alternative approach15Why data warehouse?\\n•Data warehousing employs an update driven approach in which \\ninformation from multiple, heterogeneous sources is integrated \\nin advance and stored in a warehouse for direct querying and \\nanalysis. \\n•Unlike OLTP databases, data warehouses do not contain the \\nmost current information. \\n•Data warehouse brings high performance to the integrated \\nheterogeneous database system because data are copied, \\npreprocessed, integrated, annotated, summarized, and \\nrestructured into one semantic data store .',\n",
              " 'heterogeneous database system because data are copied, \\npreprocessed, integrated, annotated, summarized, and \\nrestructured into one semantic data store .\\n⁃Query processing in data warehouses does not interfere with the \\nprocessing at local sources\\n⁃Data warehouses can store and integrate historic information and \\nsupport complex multidimensional queries. As a result, data \\nwarehousing has become popular in industry16Comparison Between Data Warehouses and OLTP \\nSystems\\nOLTP Systems Data Warehouses\\nIt holds current data and detailed \\ndata. It holds detailed data, historical data \\nand summarized data.\\nData is dynamic. Data is largely static.\\nIn this system, processing is \\nrepetitive.In this system, processing is ad hoc, \\nunstructured and heuristic.',\n",
              " 'and summarized data.\\nData is dynamic. Data is largely static.\\nIn this system, processing is \\nrepetitive.In this system, processing is ad hoc, \\nunstructured and heuristic.\\nTransaction throughput is very high. Transaction throughput is low to \\nmedium.\\nIn this case, usage pattern is \\npredictableIn this case, usage pattern is \\nunpredictable.\\nData is application oriented. Data is subject oriented.\\nIt supports day -to-day decisions. It supports strategic decisions.\\nIt serves large number of operational \\nusers.It serves relatively low number of \\nmanagerial users.17OLTP vs. OLAP\\n18OLTP vs. OLAP\\n•Users and system orientation: An OLTP system is customer -\\noriented  and is used for transaction and query processing by \\nclerks, clients, and information technology professionals.',\n",
              " '18OLTP vs. OLAP\\n•Users and system orientation: An OLTP system is customer -\\noriented  and is used for transaction and query processing by \\nclerks, clients, and information technology professionals. \\n–An OLAP system is market -oriented and is used for data analysis \\nby knowledge workers, including managers, executives, and \\nanalysts.\\n•Data contents: An OLTP system manages current data that, \\ntypically, are too detailed to be easily used for decision making.\\n–An OLAP system manages large amounts of historic data , \\nprovides facilities for summarization and aggregation, and stores \\nand manages information at different levels of granularity. \\n–These features make the data easier to use for informed decision \\nmaking.19OLTP vs. OLAP\\n•Database design: An OLTP system usually adopts an entity -',\n",
              " '–These features make the data easier to use for informed decision \\nmaking.19OLTP vs. OLAP\\n•Database design: An OLTP system usually adopts an entity -\\nrelationship (ER) data model  and an application -oriented \\ndatabase design. An OLAP system typically adopts either a \\nstar or a snowflake model and a subject -oriented database \\ndesign\\n•View:  An OLTP system focuses mainly on the current data \\nwithin an enterprise or department , without referring to historic \\ndata or data in different organizations. \\n–In contrast, an OLAP system often spans multiple versions of a \\ndatabase schema , due to the evolutionary process of an organization. \\nOLAP systems also deal with information that originates from different \\norganizations, integrating information from many data stores. Because',\n",
              " 'OLAP systems also deal with information that originates from different \\norganizations, integrating information from many data stores. Because \\nof their huge volume, OLAP data are stored on multiple storage media.20OLTP vs. OLAP\\n•Access patterns: The access patterns of an OLTP system \\nconsist mainly of short, atomic transactions . \\n•Such a system requires concurrency control and recovery \\nmechanisms. \\n•However, accesses to OLAP systems are mostly read -only \\noperations (because most data warehouses store historic \\nrather than up -to-date information), although many could be \\ncomplex queries21OLTP vs. OLAP (Summary)\\n•OLTP mainly manages current data; OLAP manages large \\namount of historical data.\\n•OLTP system consist mainly of short, atomic transactions',\n",
              " 'complex queries21OLTP vs. OLAP (Summary)\\n•OLTP mainly manages current data; OLAP manages large \\namount of historical data.\\n•OLTP system consist mainly of short, atomic transactions \\nand such a system requires concurrent control, recovery \\nmechanism, etc.\\n•OLAP system: mainly read -only operations - and dominated \\nby ad hoc complex queries.\\n•OLAP applications bulk of data can be represented in \\nmultidimensional data model.22Architecture of a Data Warehouse\\nIt has a three -tier architecture  and consists  of the \\nfollowing  components :\\n•Operational  data source\\n•Load  manager\\n•Query  manager\\n•Warehouse  manager\\n•Detailed  data\\n•Summarized  data\\n•Archive/Backup  data\\n•Metadata\\n•End-users  Access  Tools',\n",
              " 'following  components :\\n•Operational  data source\\n•Load  manager\\n•Query  manager\\n•Warehouse  manager\\n•Detailed  data\\n•Summarized  data\\n•Archive/Backup  data\\n•Metadata\\n•End-users  Access  Tools\\n•Data  Warehouse  Background  Processes  23Data Warehouse: A Multi -Tiered Architecture (3 tier)\\n24Data Warehouse: A Multi -Tiered Architecture (3 tier)\\n◼Bottom Tier: The bottom tier is a warehouse database server that is \\nalmost always a relational database system. \\n◼Back -end tools and utilities are used to feed data into the bottom tier \\nfrom operational databases or other external sources (e.g., customer \\nprofile information provided by external consultants).\\n◼These tools and utilities perform data extraction, cleaning, and',\n",
              " 'from operational databases or other external sources (e.g., customer \\nprofile information provided by external consultants).\\n◼These tools and utilities perform data extraction, cleaning, and \\ntransformation  (e.g., to merge similar data from different sources into \\na unified format), as well as load and refresh functions  to update the \\ndata warehouse. \\n◼The data are extracted using application program interfaces known \\nas gateways. A gateway (e.g., ODBC, OLEDB, JDBC etc.) is \\nsupported by the underlying DBMS and allows client programs to \\ngenerate SQL code to be executed at a server. \\n◼This tier also contains a metadata repository , which stores \\ninformation about the data warehouse and its contents.\\n 25◼Middle Tier: The middle tier is an OLAP server that is typically',\n",
              " '◼This tier also contains a metadata repository , which stores \\ninformation about the data warehouse and its contents.\\n 25◼Middle Tier: The middle tier is an OLAP server that is typically \\nimplemented using either -\\n1)a relational OLAP(ROLAP) model (i.e., an extended \\nrelational DBMS that maps operations on multidimensional \\ndata to standard relational operations); or \\n2)a multidimensional OLAP (MOLAP) model (i.e., a special -\\npurpose server that directly implements multidimensional \\ndata and operations). \\n◼Top Tier: The top tier is a front -end client layer, which contains \\nquery and reporting tools, analysis tools, and/or data mining \\ntools (e.g., trend analysis, prediction, and so on).Data Warehouse: A Multi -Tiered Architecture (3 tier)26Models of Data Warehouse\\n•Enterprise warehouse :',\n",
              " 'tools (e.g., trend analysis, prediction, and so on).Data Warehouse: A Multi -Tiered Architecture (3 tier)26Models of Data Warehouse\\n•Enterprise warehouse : \\n–Collects all of the information about subjects spanning the entire \\norganization. It provides corporate -wide data integration, usually from one \\nor more operational systems or external information providers (with \\ndetailed data as well as summarized data, having size of few gigabytes to \\nhundreds of gigabytes, terabytes, or beyond).\\n•Data Mart:\\n–A subset of corporate -wide data that is of value to a specific groups of \\nusers.  Its scope is confined to specific, selected groups, such as \\nmarketing data mart (usually implemented on low -cost departmental \\nservers). Two types of data mart: Independent  (sourced from operational',\n",
              " 'marketing data mart (usually implemented on low -cost departmental \\nservers). Two types of data mart: Independent  (sourced from operational \\nsystems or external information) and Dependent  (directly from enterprise \\nwarehouse)\\n•Virtual warehouse:\\n–A set of views over operational databases. Only some of the possible \\nsummary views may be materialized27Data Mart\\nDefinition : A subset  of a data warehouse  that supports  \\nthe requirements  of a particular  department  or business  \\nfunction  is called  data mart.\\n   Characteristics of Data Mart: \\n–A data mart focuses  on only the requirements  of users  \\nassociated  with one department  or business  function .\\n–A data mart does  not contain  detailed  operational  data.\\n–A data mart contains  less data with compared  to a data',\n",
              " 'associated  with one department  or business  function .\\n–A data mart does  not contain  detailed  operational  data.\\n–A data mart contains  less data with compared  to a data \\nwarehouse,  thus,  it is easier  to understand  and \\nnavigate . 28Approach of Data Warehouse\\nRecommended approach for data warehouse development in incremental and \\nevolutionary manner29Extraction, Transformation, & Loading (ETL)\\n•Data extraction\\n–Gathers data from multiple, heterogeneous, and external \\nsources\\n•Data cleaning\\n–Detects errors in the data and rectify them when possible\\n•Data transformation\\n–Converts data from legacy or host format to warehouse \\nformat\\n•Load\\n–Sorts, summarizes, consolidates, computes views, \\nchecks integrity, and builds indices and partitions\\n•Refresh',\n",
              " '–Converts data from legacy or host format to warehouse \\nformat\\n•Load\\n–Sorts, summarizes, consolidates, computes views, \\nchecks integrity, and builds indices and partitions\\n•Refresh\\n–Propagates the updates from the data sources to the \\nwarehouse30Metadata Repository\\nMeta data  is the data defining warehouse objects. Meta data  \\nrepositories stores:\\n•Description of the structure  of the data warehouse\\n–schema, view, dimensions, hierarchies, derived data defn, data \\nmart locations and contents\\n•Operational  meta -data\\n–data lineage (history of migrated data and transformation path), \\ncurrency of data (active, archived, or purged), monitoring \\ninformation (warehouse usage statistics, error reports, audit trails)\\n•The algorithms  used for summarization',\n",
              " 'currency of data (active, archived, or purged), monitoring \\ninformation (warehouse usage statistics, error reports, audit trails)\\n•The algorithms  used for summarization\\n•The mapping  from operational environment to the data warehouse\\n•Data related to system performance\\n–warehouse schema, view and derived data definitions\\n•Business metadata\\n–business terms and definitions, ownership of data, charging policies31OLAP Tools\\n•Multi -dimensional  OLAP  (MOLAP)  – These  tools  utilize  \\nspecialized  data structures  and multidimensional  database  \\nmanagement  systems  to organize,  navigate  and analysis  \\ndata.\\n \\n•Relational  OLAP  (ROLAP)  - These  tools  support  RDBMS  \\nproducts  directly  through  a dictionary  layer  of metadata,',\n",
              " 'management  systems  to organize,  navigate  and analysis  \\ndata.\\n \\n•Relational  OLAP  (ROLAP)  - These  tools  support  RDBMS  \\nproducts  directly  through  a dictionary  layer  of metadata,  \\nthereby  avoiding  the requirement  to create  a static  \\nmultidimensional  data structure . \\n•Hybrid  OLAP  (HOLAP)  - These  tools  provide  limited  \\nanalysis  capability,  either  directly  against  RDBMS  products,  \\nor by leveraging  an intermediate  MOLAP  server . 32On-line Analytical Processing (OLAP)\\nThe on-line analytical  processing  represents  a technology  that uses  a \\nmultidimensional  view of aggregated  data to provide  a quick  access  \\nto strategic  information  for performing  advanced  analysis . Codd’s  \\ntwelve  rules  for selecting  OLAP  tools :',\n",
              " 'multidimensional  view of aggregated  data to provide  a quick  access  \\nto strategic  information  for performing  advanced  analysis . Codd’s  \\ntwelve  rules  for selecting  OLAP  tools :\\n–Multidimensional  conceptual  view \\n–Transparency\\n–Accessibility\\n–Consistent  reporting  performance  \\n–Client/Server  architecture\\n–Generic  dimensionality  \\n–Dynamic  sparse  matrix  handling  \\n–Multi -user support\\n–Unrestricted  cross -dimensional  operations  \\n–Intuitive  data manipulation\\n–Flexible  reporting\\n–Unlimited  dimensions  and aggregation  levels  33•A data cube allows data to be modeled and viewed in multiple \\ndimensions. It is defined by dimensions  and facts .\\n–Dimension: are the perspectives or entities w.r.t.  which the',\n",
              " 'dimensions. It is defined by dimensions  and facts .\\n–Dimension: are the perspectives or entities w.r.t.  which the \\norganization want to keep records. For example, time, branch, item \\netc. A dimension table for item may contain the attributes item name, \\nbrand, and type .\\n–Facts: are numeric measures. It contains the names of the facts, or \\nmeasures, as well as keys to each of the related dimension tables. \\nExamples of facts for a sales data warehouse include dollars sold, \\nunits sold, and amount budgeted.\\n•In multiple data model, data are organized into multiple \\ndimensions, an each dimension contains multiple levels of \\nabstractions.\\n•Various multidimensional models are: star schema, snowflake \\nschema, and fact constellation\\n•This allows users to view data from different angles.',\n",
              " 'abstractions.\\n•Various multidimensional models are: star schema, snowflake \\nschema, and fact constellation\\n•This allows users to view data from different angles.\\n•In data warehousing the data cube is n -dimensional.Data Cube (as a multidimensional data)34Data Cube (as a multidimensional data)\\n•A number of OLAP data cube operations exists to materialize \\nthe different views.\\n•A data warehouse  is based on a multidimensional data \\nmodel  which views data in the form of a data cube\\n•A data cube, such as sales , allows data to be modeled and \\nviewed in multiple dimensions\\n–Dimension tables , such as item ( item_name , brand, type), time (day, \\nweek, month, quarter, year), and sales ( item, branch, location ) \\n–The fact table contains the names of the facts, or measures, (such as',\n",
              " 'week, month, quarter, year), and sales ( item, branch, location ) \\n–The fact table contains the names of the facts, or measures, (such as \\ndollars_sold ) and keys to each of the related dimension tables35•A cuboid  can be generated for each of the possible subsets of \\nthe given set of dimensions.\\n•The result would form a lattice of cuboids, each showing the \\ndata at a different level of summarization, or group -by. The \\nlattice  of cuboids forms a data cube .\\n•The lattice of cuboids is then referred to as a data cube.\\n•Representing data cubes (in previous slides, e.g., 2 -D data \\ncube, 3 -D data cube, 4 -D data cube etc.) are called data \\ncuboids . In data warehousing literature, an n-D base cube \\n(holding the lowest level of summarization ) is called a base \\ncuboid .',\n",
              " 'cube, 3 -D data cube, 4 -D data cube etc.) are called data \\ncuboids . In data warehousing literature, an n-D base cube \\n(holding the lowest level of summarization ) is called a base \\ncuboid . \\n•The top most 0-D cuboid , which holds the highest -level of \\nsummarization , is called the apex cuboid .  Data Cube (as a multidimensional data)36Cube: A Lattice of Cuboids\\n37Conceptual Modeling of Data Warehouses\\n•The E -R data model is commonly used in the design of relational \\ndatabases, where a database schema consists of a set of entities \\nand the relationships between them\\n•Multidimensional modeling of data warehouses requires: \\ndimensions & measures (named as facts)\\n–Star schema : A fact table in the middle connected to a set of',\n",
              " 'and the relationships between them\\n•Multidimensional modeling of data warehouses requires: \\ndimensions & measures (named as facts)\\n–Star schema : A fact table in the middle connected to a set of \\ndimension tables. Attributes within a dimension table may form either \\na hierarchy (total order) or a lattice (partial order). \\n–Snowflake schema :  It is a variant of the star schema model. A \\nrefinement of star schema where some dimensional hierarchy is \\nnormalized  into a set of smaller dimension tables (to reduce \\nredundancies), forming a shape similar to snowflake\\n–Fact constellations :  Multiple fact tables share dimension tables , \\nviewed as a collection of stars, therefore called galaxy schema  or fact \\nconstellation 38A Concept Hierarchy: Dimension  (location)\\nMexicoall',\n",
              " 'viewed as a collection of stars, therefore called galaxy schema  or fact \\nconstellation 38A Concept Hierarchy: Dimension  (location)\\nMexicoall\\nEurope North_America\\nCanada Spain Germany\\nVancouver\\nM. Wind L. Chan...\\n... ...\\n... ...\\n...all\\nregion\\nofficecountry\\nToronto Frankfurt city39Data Cube Measurement : Three Categories\\n(based on the kind of aggregate functions used)\\n•Distributive : Data are partitioned into n sets. Then a function (say \\naggregate ) is applied to each partition, resulting in n aggregate \\nvalues. If the result derived by applying the function to n aggregate \\nvalues is the same as that derived by applying the function on all the \\ndata without partitioning\\n•E.g., count(), sum(), min(), max()\\n•Algebraic : An aggregate function is algebraic if it can be computed',\n",
              " 'data without partitioning\\n•E.g., count(), sum(), min(), max()\\n•Algebraic : An aggregate function is algebraic if it can be computed \\nby an algebraic function with M arguments (where  M is a bounded \\ninteger), each of which is obtained by applying a distributive \\naggregate function\\n•E.g.,  avg(), min_N (), max_N (), standard_deviation ()\\n•Holistic : An aggregate function is holistic if there is no constant \\nbound on the storage size needed to describe a sub -aggregate  \\n•E.g., median(), mode(), rank()40Multidimensional Data\\n•Sales volume as a function of product, month, and region\\nProduct\\nMonthDimensions: Product, Location, Time\\nHierarchical summarization paths\\nIndustry   Region          Year\\nCategory   Country  Quarter\\nProduct      City     Month    Week',\n",
              " 'Product\\nMonthDimensions: Product, Location, Time\\nHierarchical summarization paths\\nIndustry   Region          Year\\nCategory   Country  Quarter\\nProduct      City     Month    Week\\n                   Office         Day\\n Hierarchies for product       Lattice for time\\n            & location41A Sample Data Cube\\nTotal annual sales\\nof  TVs in U.S.A.Date\\nCountry\\nAll, All, AllsumsumTV\\nVCRPC1Qtr 2Qtr 3Qtr 4Qtr\\nU.S.A\\nCanada\\nMexico\\nsum42Cuboids Corresponding to the Cube\\nall\\nproductdatecountry\\nproduct,date\\nproduct,countrydate, country\\nproduct, date, country0-D (apex ) cuboid\\n1-D cuboids\\n2-D cuboids\\n3-D (base ) cuboid43OLAP Operations\\n•Roll up (drill -up): (to) summarize data\\n–by climbing up hierarchy or by dimension reduction\\n•Drill down (roll down):  reverse of roll -up',\n",
              " '1-D cuboids\\n2-D cuboids\\n3-D (base ) cuboid43OLAP Operations\\n•Roll up (drill -up): (to) summarize data\\n–by climbing up hierarchy or by dimension reduction\\n•Drill down (roll down):  reverse of roll -up\\n–from higher level summary to lower level summary or detailed \\ndata, or introducing new dimensions\\n•Slice and dice:  project (synonymous to projection) and select  \\n•Pivot (rotate):  rotate the data axes in view to provide an \\n         alternative data presentation\\n–reorient the cube (i.e., rotating the axes in a 3 -D cube), \\nvisualization, transforming 3D cube to series of 2D planes etc.\\n•Other OLAP operations:\\n–drill across:  involving (across) more than one fact table\\n–drill through:  through the bottom level of the cube to its back -end \\nrelational tables (using SQL)44Data Warehouse Usage',\n",
              " '–drill across:  involving (across) more than one fact table\\n–drill through:  through the bottom level of the cube to its back -end \\nrelational tables (using SQL)44Data Warehouse Usage\\n•Three kinds of data warehouse applications:\\n–Information processing\\n•supports querying, basic statistical analysis, and reporting \\nusing crosstabs, tables, charts and graphs\\n–Analytical processing\\n•multidimensional analysis of data warehouse data\\n•supports basic OLAP operations, slice -dice, drilling, pivoting\\n–Data mining\\n•knowledge discovery from hidden patterns \\n•supports associations, constructing analytical models, \\nperforming classification and prediction, and presenting the \\nmining results using visualization tools45Problems in Data Warehousing\\n•Complexity of data integration',\n",
              " 'performing classification and prediction, and presenting the \\nmining results using visualization tools45Problems in Data Warehousing\\n•Complexity of data integration\\n•Hidden problems with data sources\\n•Required data not captured\\n•Underestimation of time for data loading \\n•Increased storage space\\n•High maintenance\\n•Long -duration projects\\n•Data homogenization 46Benefits of Data warehousing \\n•Returns on investment in data warehousing is very high. \\n•The competitive advantage is gained from data warehousing by \\nallowing decision -makers access to data that can reveal \\npreviously unavailable, unknown and not captured. \\n•Data warehousing improves the productivity of corporate \\ndecision -makers. A data warehouse helps business managers',\n",
              " 'previously unavailable, unknown and not captured. \\n•Data warehousing improves the productivity of corporate \\ndecision -makers. A data warehouse helps business managers \\nto perform more substantive, accurate and consistent analysis.\\n•Data warehousing reduces the redundant processing and \\nprovides software to support overlapping decision support \\napplications. \\n•A data warehouse facilitates better business intelligence by \\nincreased quality and flexibility of market analysis available \\nthrough multi -level data structures. \\n•It also facilitates business process reengineering.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1WK54-74Rpt"
      },
      "source": [
        "### Load the dataset into the vector store\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5BECsdSUUM",
        "outputId": "818e40dd-bc95-4c47-ffbc-b68c17fc76db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 40 headlines.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "astra_vector_store.add_texts(texts[:50])\n",
        "\n",
        "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
        "\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhVf0kir2Uke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLJp8yPF4Rpt"
      },
      "source": [
        "### Run the QA cycle\n",
        "\n",
        "Simply run the cells and ask a question -- or `quit` to stop. (you can also stop execution with the \"▪\" button on the top toolbar)\n",
        "\n",
        "Here are some suggested questions:\n",
        "- _What is a data warehouse?\n",
        "- What is the difference between oltp and olap?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJugrh7SX3C",
        "outputId": "0543b4c2-9152-46ee-e22b-9a7f9cba532a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter your question (or type 'quit' to exit): what is data ware house?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "QUESTION: \"what is data ware house?\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANSWER: \"A data warehouse is a multi-tiered architecture that collects and stores data from various sources, providing a centralized repository for business intelligence and analytics. It is designed to support business decision-making by integrating data from operational systems, external sources, and other data sources.\n",
            "\n",
            "A data warehouse is typically characterized as follows:\n",
            "\n",
            "* It collects all information about a subject spanning the entire organization.\n",
            "* It provides corporate-wide data integration, usually from one or more operational systems or external information providers.\n",
            "* It contains detailed data as well as summarized data, with a size ranging from a few gigabytes to hundreds of gigabytes, terabytes, or beyond.\n",
            "\n",
            "Data warehouses improve the productivity of corporate decision-makers by enabling them to perform more substantive, accurate, and consistent analysis. They also reduce redundant processing and provide software to support overlapping decision support applications.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.7994] \"tools (e.g., trend analysis, prediction, and so on).Data Warehouse: A Multi -Tiered  ...\"\n",
            "    [0.7967] \"previously unavailable, unknown and not captured. \n",
            "•Data warehousing improves the pr ...\"\n",
            "    [0.7879] \"heterogeneous database system because data are copied, \n",
            "preprocessed, integrated, an ...\"\n",
            "    [0.7815] \"marketing data mart (usually implemented on low -cost departmental \n",
            "servers). Two ty ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): quit\n"
          ]
        }
      ],
      "source": [
        "first_question = True\n",
        "while True:\n",
        "    if first_question:\n",
        "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "    else:\n",
        "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "    if query_text.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    if query_text == \"\":\n",
        "        continue\n",
        "\n",
        "    first_question = False\n",
        "\n",
        "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
        "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSaUPguw389l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}